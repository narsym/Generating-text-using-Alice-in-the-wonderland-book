{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Text(one to many).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPz6rBVRcuJSu2W2T0gLsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narsym/Generating-text-using-Alice-in-the-wonderland-book/blob/master/Generate_Text(one_to_many).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Ddlf3ADbhf",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyT9bQmeBEMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcilE9vyDdyJ",
        "colab_type": "text"
      },
      "source": [
        "Directories for data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJk_rv3NC6HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"./data\"\n",
        "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1HTBMZpDf5Z",
        "colab_type": "text"
      },
      "source": [
        "Making the directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w9mh9CBDHK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir(DATA_DIR)\n",
        "os.mkdir(CHECKPOINT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF-2DO0_Dy6y",
        "colab_type": "text"
      },
      "source": [
        "Download the data and preprocess a little"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwcT2UrTDNb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read(urls):\n",
        "    texts = []\n",
        "    for i, url in enumerate(urls):\n",
        "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n",
        "            cache_dir=\".\")\n",
        "        text = open(p, \"r\").read()\n",
        "        # remove byte order mark\n",
        "        text = text.replace(\"\\ufeff\", \"\")\n",
        "        # remove newlines\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = re.sub(r'\\s+', \" \", text)\n",
        "        # add it to the list\n",
        "        texts.extend(text)\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiBzClxUE87V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = download_and_read([\n",
        "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
        "    \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsXSsNV-FGp9",
        "colab_type": "text"
      },
      "source": [
        "We create some mapping dictionaries to convert each vocabulary character to a unique integer and vice versa. The input and output of the network is a sequence of characters. However, the actual input and output of the network are sequences of integers, and we will use these mapping dictionaries to handle this conversion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KN1XkpNK5jy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c67d2430-b186-4261-ee30-7f8caed5d4cf"
      },
      "source": [
        "#create the vocabulary\n",
        "vocab = sorted(set(texts))\n",
        "print(\"vocab size: {:d}\".format(len(vocab)))\n",
        "\n",
        "# create mapping from vocab chars to ints\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "idx2char = {i:c for c, i in char2idx.items()}"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2ihLqtLrK8",
        "colab_type": "text"
      },
      "source": [
        "Converting the characters to ints and making them into tf.data.Dataset objects. Batch the dataset into a slices of 101 characters and apply split_train_labels() to create sequences dataset, which is a dataset of tuples of two elements, each element of the tuple being a vector of size 100 and type tf.int64. We then shuffle these sequences and then create batches of 64 tuples each for input to our network. Each element of the dataset is now a tuple consisting of a pair of matrices, each of size (64, 100) and type tf.int64:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIKxOJSPMuun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d1a3f20-6ffe-4b3b-e2a5-b3bc26100b72"
      },
      "source": [
        "# numericize the texts\n",
        "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
        "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
        "\n",
        "# number of characters to show before asking for prediction\n",
        "# sequences: [None, 100]\n",
        "seq_length = 100\n",
        "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "\n",
        "def split_train_labels(sequence):\n",
        "    input_seq = sequence[0:-1]\n",
        "    output_seq = sequence[1:]\n",
        "    return input_seq, output_seq\n",
        "\n",
        "sequences = sequences.map(split_train_labels)\n",
        "#setting up for training\n",
        "#batches [None, 64, 100]\n",
        "batch_size = 64\n",
        "steps_per_epoch = len(texts) // seq_length // batch_size\n",
        "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHsCqzvUQux1",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to define our network. As before, we define our network as a subclass of tf.keras.Model as shown next. The network is fairly simple; it takes as input a sequence of integers of size 100 (num_timesteps) and passes them through an Embedding layer so that each integer in the sequence is converted to a vector of size 256 (embedding_dim). So, assuming a batch size of 64, for our input sequence of size (64, 100), the output of the Embedding layer is a matrix of shape (64, 100, 256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_4TkRLfRW3a",
        "colab_type": "text"
      },
      "source": [
        "The next layer is the RNN layer with 100 time steps. The implementation of RNN chosen is a GRU. This GRU layer will take, at each of its time steps, a vector of size (256,) and output a vector of shape (1024,) (rnn_output_dim). Note also that the RNN is stateful, which means that the hidden state output from the previous training epoch will be used as input to the current epoch. The return_sequences=True flag also indicates that the RNN will output at each of the time steps rather than an aggregate output at the last time steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUCpP6fgRaV5",
        "colab_type": "text"
      },
      "source": [
        "Finally, each of the time steps will emit a vector of shape (1024,) into a Dense layer that outputs a vector of shape (90,) (vocab_size). The output from this layer will be a tensor of shape (64, 100, 90). Each position in the output vector corresponds to a character in our vocabulary, and the values correspond to the probability of that character occurring at that output position:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExfaZiiXRdoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "d5e213b8-fa2b-42c6-d80e-fa3f53f06a94"
      },
      "source": [
        "class CharGenModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, num_timesteps, \n",
        "            embedding_dim, **kwargs):\n",
        "        super(CharGenModel, self).__init__(**kwargs)\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim\n",
        "        )\n",
        "        self.rnn_layer = tf.keras.layers.GRU(\n",
        "            num_timesteps,\n",
        "            recurrent_initializer=\"glorot_uniform\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            stateful=True,\n",
        "            return_sequences=True\n",
        "        )\n",
        "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = self.rnn_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        return x\n",
        "# define network\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "\n",
        "model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
        "model.build(input_shape=(batch_size, seq_length))\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"char_gen_model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      multiple                  23040     \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  multiple                  107400    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              multiple                  9090      \n",
            "=================================================================\n",
            "Total params: 139,530\n",
            "Trainable params: 139,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6g7u_WUuaf",
        "colab_type": "text"
      },
      "source": [
        "Next we define a loss function and compile our model. We will use the sparse categorical cross-entropy as our loss function because that is the standard loss function to use when our inputs and outputs are sequences of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNFrciSfb8pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb1c45eb-44ba-4fe2-c685-efd3ae8b56f9"
      },
      "source": [
        "for input_batch, label_batch in dataset.take(1):\n",
        "    pred_batch = model(input_batch)\n",
        "\n",
        "print(pred_batch.shape)\n",
        "assert(pred_batch.shape[0] == batch_size)\n",
        "assert(pred_batch.shape[1] == seq_length)\n",
        "assert(pred_batch.shape[2] == vocab_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxGQFvuNU91a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, predictions):\n",
        "    return tf.losses.sparse_categorical_crossentropy(\n",
        "        labels,\n",
        "        predictions,\n",
        "        from_logits=True\n",
        "    )\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmr3jbnVTPz",
        "colab_type": "text"
      },
      "source": [
        "Normally, the character at each position of the output is found by computing the argmax of the vector at that position, that is, the character corresponding to the maximum probability value. This is known as greedy search. In the case of language models where the output of one timestep becomes the input to the next timestep, this can lead to repetitive output. The two most common approaches to overcome this problem is either to sample the output randomly or to use beam search, which samples from k the most probable values at each time step. Here we will use the tf.random.categorical() function to sample the output randomly. The following function takes a string as a prefix and uses it to generate a string whose length is specified by num_chars_to_generate. The temperature parameter is used to control the quality of the predictions. Lower values will create a more predictable output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl3ylIeGWSrN",
        "colab_type": "text"
      },
      "source": [
        "The logic follows a predictable pattern. We convert the sequence of characters in our prefix_string into a sequence of integers, then expand_dims to add a batch dimension so the input can be passed into our model. We then reset the state of the model. This is needed because our model is stateful, and we don't want the hidden state for the first timestep in our prediction run to be carried over from the one computed during training. We then run the input through our model and get back a prediction. This is the vector of shape (90,) representing the probabilities of each character in the vocabulary appearing at the next time step. We then reshape the prediction by removing the batch dimension and dividing by the temperature, then randomly sample from the vector. We then set our prediction as the input to the next time step. We repeat this for the number of characters we need to generate, converting each prediction back to character form and accumulating in a list, and returning the list at the end of the loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtEDN7eLWWSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, prefix_string, char2idx, idx2char,\n",
        "        num_chars_to_generate=1000, temperature=1.0):\n",
        "    input = [char2idx[s] for s in prefix_string]\n",
        "    input = tf.expand_dims(input, 0)\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "    for i in range(num_chars_to_generate):\n",
        "        preds = model(input)\n",
        "        preds = tf.squeeze(preds, 0) / temperature\n",
        "        # predict char returned by model\n",
        "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
        "        text_generated.append(idx2char[pred_id])\n",
        "        # pass the prediction as the next input to the model\n",
        "        input = tf.expand_dims([pred_id], 0)\n",
        "\n",
        "    return prefix_string + \"\".join(text_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jXuH353X0pU",
        "colab_type": "text"
      },
      "source": [
        "Training and generating the text at the end of every 10 epochs and always prefix is Alice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZY4c7z7YL4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0aaebfa9-4463-4392-d740-7ce6b9162a2d"
      },
      "source": [
        "# we will train our model for 50 epochs, and after every 10 epochs\n",
        "# we want to see how well it will generate text\n",
        "num_epochs = 50\n",
        "for i in range(num_epochs // 10):\n",
        "    model.fit(\n",
        "        dataset.repeat(),\n",
        "        epochs=10,\n",
        "        steps_per_epoch=steps_per_epoch\n",
        "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
        "    )\n",
        "    checkpoint_file = os.path.join(\n",
        "        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
        "    model.save_weights(checkpoint_file)\n",
        "\n",
        "    # create a generative model using the trained model so far\n",
        "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
        "    gen_model.load_weights(checkpoint_file)\n",
        "    gen_model.build(input_shape=(1, seq_length))\n",
        "\n",
        "    print(\"after epoch: {:d}\".format(i+1)*10)\n",
        "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
        "    print(\"---\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 3.5034\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.7648\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.4822\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.3355\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.2313\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.1360\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 2.0617\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.9931\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.9417\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.8870\n",
            "after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1\n",
            "Alice cat shaill fillome tontertinber--it?’ gontt appericuple grake in, and had of coll! \"You keld litt bed xaght,\" said lannel whin paather one for looked?' [Anscen,’ stile of thing with't tre of the hishtly her and anremting a leonce out, youl ittle a kyough’ll omn Grealder go in to her woust of tildan'ts at’stlill as magaco the realrry thain whboucher. ‘Your, Buteec it you cavist jarperout THat bact: been your hism, I saddryict it the and the soid the some be Lim puppen the jrypwat yepmenthen!\" DOMI’ Aich o’tt get Alice Red plown, of waid he Theich seens. Alice uton agaly begt, don, afl of the \"pan, thated any the somt, that us unes.\" \"fom the Cad. Theres all a mo hay bet at cecthpremver and so liys slead a sindant some. I Wabying fistly marking the hay it when tor geif up ditt Alice som to quith looke saud ovnow whit'rbut,’ said stat of I ne’ll; and the Queen aldode ret?\" Delmont till and you litting be a muph it apithle Dumnednen; of lin't cumder hand so the: sill Pro/jurks I'm at's abb\n",
            "---\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.8466\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.8077\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.7747\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.7434\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.7198\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.6888\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.6746\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.6481\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.6341\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.6165\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2\n",
            "Alice Mays gome wouldn’t oP? If is have see to having that at starks of the kitsatice,\" she went tond a quect crowe in the doment with it I tame said:; Messo with here?’ ‘That look it as she began out?’ ‘Then’s back, ‘ife here it madteart was queers about in just it as he said, be for the Projutteng-tm one were not in the onee he story bright bettrong, \"you cavemade or temmares, and deas, id guch teoled the boak inderst,\" Alice coulded in a please after and beed al?’ ‘I’ll have not,\" said ghemstle can looking-twick CHACTH” inse more-bol offe enough facked for trat. ‘Bety for the rictht to ome had “ME way’s that’s and o man,\" said she say fort of pare Squice, in, por--air, and the Reck book betneres as masthing vere gon the frow deLarthan geres, as manmed on it brot into that aftious. 1.F1.% that of inedly, when she Caterberg-tm long.’ Alice was mabtemed in the Red one a minute, if she tredledever!\" Torsicutled was she ropy it up she Walk, cas ised, _what you knearday quite more the who, \"Mem\n",
            "---\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.6003\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.5862\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.5709\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.5595\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.5497\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.5418\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.5249\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.5193\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.5146\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4998\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3\n",
            "Alice said, after to gees and not, as came the gnasself, as talk, that clown, ‘and he Bud thupsion. \"It's implying exe-tope Dining Li1.1. A by all you relading a little electrocking quicely!\" I one of perslow learge how in had cauming anw!\" said Alice other offigh VERY me.\" And the Knight time; I'm unchessing me said he could Paject Gutene. Do, than or till the knightenectilight very to hand you’ve or all to never-a\" said thing, \"I get whing; ‘jreat--and all the getterution. \"Loblion, and flass?\"S an the Gryphon't come--wh dies. Whying \"I gelimitely gure ismanting on. I’m, while they had a popty wee VERY ramnsoffer, wen Have make Lieest, and bace. ‘Let in the Mock Turiol!\" \"Do tall she all the tried: ‘to he said \"veryt muchfouror.’ ‘As loking went on-' Why: \"up again,\" as up I talk--Now,’ she shormeed of and nginelind. Saven, as ho see it as muld rASquice after a passpression.\" \"Then it! The Mocty coph: but at mape. \"I shayh shat Alice asked A got turn in she said to you? I wad a nothing as \n",
            "---\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4951\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4871\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4802\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4708\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4709\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4552\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4575\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4483\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4437\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4379\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4\n",
            "Alice tfice. \"T LiFaje----at leadaregohed on. The carry, mind. She went dret of Alice; \"I chuphing docitus so fight for the Caterpiss!’ the Hatter is. It’s she have the Pare,’ said Alice. \"Ming herself M9I[E. It que-fiethed well, if, That the Project Gutenberg that is to her things fell herself in a nice. \"Me the Foundatificwarts, by a was “There, Hover the fast begin about it: the Marche Queen.L Dustrabigo,” any aLre hearring in the aspery shoum-flost to her, who had not Yeh, chonard mousom as her preatard rettended, and doush pixhing, but tumid, Curtledee. \"Then him baking and things left help begin go,’ the Guce are as lescertation is very bright to yout her quite pair Mif comply. She would herself talking work as you muce. It was side, and she Dumpty) Sheare rattered to her from?’ Alice was the winglating or them into e.’ Telt, and till you don’t be andwever im any wat going for closition: \"crows into Alice, and he roning gwopped for short jouch,’ Alice same interrumprise--that it as got\n",
            "---\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4333\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4327\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4244\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4215\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4179\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4135\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4075\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4065\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.3989\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 1.4001\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5\n",
            "Alice till she sing up to get catcepty gress were agravy. ‘I khouk “Hant through the oacle in the was bronater on it: ‘there HAS A Butters.’ ‘Dinah-box paid the felmout were there’s got up; it away on was dough that hur's time. The is Sheend their tell into house-pass.’ (Alice trawar!\" There lovely,’ the whiten watching about puzzled Igate on it was she Knigst couldn’t helving have crajesty saal of grinned a vight to be at beaclect as he do al Bute of this works, copyrowon a nise, and sent to herself, will you’ll way any offed and with his hand good too lightic usel when the Whith a kett: OF happen, she went lessons!\" said Alice, becauded to her moust's thought's head. Alice half foonche Queen as she foun.’ ‘You did, and she went on. ‘I’ll use; but book RAR1_, I munder again. ‘It’s another, I shriggudly explanation on marked a little particul or Project Gutenberg-tm eyes from mued convers and that,\" said The March lively was gringen eager!\" But Alice Queen added, he side at the dristly in a \n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}